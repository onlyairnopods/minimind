"""
MiniMind çš„ Transformer Block æ¶æ„è®¾è®¡ä¸ä»…å…‹åˆ¶ï¼Œæ›´é›†æˆäº†å½“å‰å¤§æ¨¡å‹æœ€å‰æ²¿çš„å·¥ç¨‹åŒ–æŠ€å·§ï¼š

- RoPE & YaRN (Dynamic Scaling)ï¼š ä»£ç ä¸­ä¸ä»…å®ç°äº†æ ‡å‡†çš„æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼Œæ›´å†…åµŒäº† YaRN (Yet another RoPE extensioN) ç®—æ³•ã€‚
é€šè¿‡åŠ¨æ€è°ƒæ•´é¢‘ç‡ï¼ˆramp å‡½æ•°ï¼‰ï¼Œä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¨ç†æ—¶çªç ´è®­ç»ƒé•¿åº¦é™åˆ¶ï¼ˆå¦‚ä» 2k å¤–æ¨è‡³ 32kï¼‰ï¼Œå®ç°äº†â€œè®­ç»ƒçŸ­ï¼Œæ¨ç†é•¿â€çš„é«˜æ•ˆç­–ç•¥ã€‚

- Pre-Norm RMSNormï¼š æ‘’å¼ƒäº†ä¼ ç»Ÿ LayerNorm çš„ä¸­å¿ƒåŒ–æ“ä½œï¼Œä»…ä¿ç•™ç¼©æ”¾ï¼Œç»“åˆ Pre-Norm ç»“æ„æ˜¾è‘—æå‡äº†æ·±å±‚ç½‘ç»œçš„è®­ç»ƒç¨³å®šæ€§ä¸æ”¶æ•›é€Ÿåº¦ã€‚

- GQA + Flash Attentionï¼š é‡‡ç”¨äº†åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›ï¼ˆGQAï¼‰ï¼Œå¤§å¹…å‹ç¼©äº† KV Cache çš„æ˜¾å­˜å ç”¨ï¼›
åŒæ—¶åœ¨åº•å±‚è‡ªåŠ¨é€‚é… PyTorch çš„ F.scaled_dot_product_attentionï¼Œæ ¹æ®ç¯å¢ƒè‡ªåŠ¨å¯ç”¨ Flash Attention åŠ é€Ÿï¼Œå®ç°äº†æ˜¾å­˜ä¸è®¡ç®—çš„åŒé‡ä¼˜åŒ–ã€‚

- SwiGLU / Hybrid MoEï¼š å‰é¦ˆç½‘ç»œä¸ä»…ä½¿ç”¨äº† GLU é—¨æ§æœºåˆ¶ï¼Œæ›´åœ¨ MoE æ¨¡å¼ä¸‹é‡‡ç”¨äº† Hybridï¼ˆæ··åˆï¼‰ä¸“å®¶æ¶æ„ï¼ˆn_shared_experts + n_routed_expertsï¼‰ã€‚
è¿™ç§â€œå…±äº«ä¸“å®¶è´Ÿè´£é€šç”¨çŸ¥è¯†ï¼Œè·¯ç”±ä¸“å®¶è´Ÿè´£å‚ç±»çŸ¥è¯†â€çš„è®¾è®¡ï¼ˆç±»ä¼¼ DeepSeek-MoEï¼‰ï¼Œé…åˆ Aux Loss è´Ÿè½½å‡è¡¡ï¼Œæå¤§åœ°æå‡äº†æ¨¡å‹çš„éçº¿æ€§è¡¨è¾¾èƒ½åŠ›ä¸å‚æ•°åˆ©ç”¨ç‡ã€‚

- Weight Tying & Vocab Compressionï¼š é™¤äº†ç²¾ç®€è¯è¡¨å¤–ï¼ŒMiniMindForCausalLM ä¸­æ˜¾å¼æ‰§è¡Œäº† embed_tokens.weight = lm_head.weight çš„ æƒé‡ç»‘å®šï¼ˆWeight Tyingï¼‰ã€‚
è¿™ä¸€æŠ€å·§è®©è¾“å…¥ Embedding ä¸è¾“å‡º Head å…±äº«å‚æ•°ï¼Œåœ¨å°å‚æ•°é‡æ¨¡å‹ä¸­èƒ½æ˜¾è‘—å‡å°‘å†—ä½™ï¼Œç¡®ä¿æ¯ä¸€åˆ†å‚æ•°é¢„ç®—éƒ½ç”¨åœ¨â€œåˆ€åˆƒâ€ä¸Šã€‚
"""


# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Config
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

from transformers import PretrainedConfig


class MiniMindConfig(PretrainedConfig):
    model_type = "minimind"

    def __init__(
            self,
            dropout: float = 0.0,
            bos_token_id: int = 1,
            eos_token_id: int = 2,
            hidden_act: str = 'silu',
            hidden_size: int = 512,
            intermediate_size: int = None, # FFN ä¸­é—´å±‚ç»´åº¦
            max_position_embeddings: int = 32768,
            num_attention_heads: int = 8, # Query çš„å¤´æ•°
            num_hidden_layers: int = 8,
            num_key_value_heads: int = 2, # Key/Value çš„å¤´æ•° (æ¶‰åŠ GQA) å½“æ­¤å€¼å°äº num_attention_heads æ—¶ï¼Œå³å¼€å¯äº† GQA (åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›)ã€‚è¿™é‡Œ 8 ä¸ª Q å¤´å…±äº« 2 ç»„ KV å¤´ï¼ˆ4:1ï¼‰ï¼Œèƒ½æ˜¾è‘—é™ä½æ¨ç†æ˜¾å­˜å ç”¨ã€‚
            vocab_size: int = 6400,
            rms_norm_eps: float = 1e-05,
            rope_theta: int = 1000000.0,
            inference_rope_scaling: bool = False,
            flash_attn: bool = True,
            ####################################################
            # Here are the specific configurations of MOE
            # When use_moe is false, the following is invalid
            ####################################################
            use_moe: bool = False,
            num_experts_per_tok: int = 2, # Top-K è·¯ç”±æ•°ã€‚æ¯ä¸ª Token åœ¨æ¨ç†æ—¶å®é™…ä¼šæ¿€æ´»çš„è·¯ç”±ä¸“å®¶æ•°é‡ã€‚å°½ç®¡æ€»ä¸“å®¶å¤šï¼Œä½†æ¯ä¸ª Token åªè®¡ç®—è¿™ 2 ä¸ªï¼Œä¿è¯äº†æ¨ç†é€Ÿåº¦ã€‚
            n_routed_experts: int = 4, # è·¯ç”±ä¸“å®¶æ€»æ•°ã€‚å¯ä¾›é€‰æ‹©çš„ä¸“ç”¨ä¸“å®¶æ€»æ•°é‡ã€‚
            n_shared_experts: int = 1, # å…±äº«ä¸“å®¶æ•°é‡ã€‚æ— è®ºè·¯ç”±ç»“æœå¦‚ä½•ï¼Œæ‰€æœ‰ Token å¿…ç„¶ä¼šç»è¿‡çš„ä¸“å®¶ã€‚ç”¨äºæ•æ‰é€šç”¨çŸ¥è¯†ï¼ˆè¿™æ˜¯ DeepSeek-MoE æ¶æ„çš„å…¸å‹ç‰¹å¾ï¼‰ã€‚
            scoring_func: str = 'softmax', # é—¨æ§è¯„åˆ†å‡½æ•°ã€‚Router ç½‘ç»œä½¿ç”¨ Softmax æ¥è®¡ç®—æ¯ä¸ªä¸“å®¶çš„æƒé‡æ¦‚ç‡ã€‚
            aux_loss_alpha: float = 0.01, # è¾…åŠ©æŸå¤±ç³»æ•°ã€‚è®­ç»ƒæ—¶çš„è´Ÿè½½å‡è¡¡æƒ©ç½šé¡¹æƒé‡ã€‚é˜²æ­¢ Router æ€»æ˜¯åªé€‰æŸå‡ ä¸ªä¸“å®¶ï¼ˆå¯¼è‡´ä¸“å®¶åå¡Œï¼‰ï¼Œå¼ºåˆ¶è®©æ‰€æœ‰ä¸“å®¶éƒ½â€œå¿™èµ·æ¥â€ã€‚
            seq_aux: bool = True, # åºåˆ—çº§è¾…åŠ©æŸå¤±ã€‚è®¡ç®—è¾…åŠ©æŸå¤±çš„èŒƒå›´æ˜¯åœ¨æ•´ä¸ªåºåˆ—çº§åˆ«ä¸Šç»Ÿè®¡ï¼Œè€Œéä»…é’ˆå¯¹å•ä¸ª Tokenã€‚
            norm_topk_prob: bool = True, # æ¦‚ç‡å½’ä¸€åŒ–ã€‚é€‰å‡º Top-K ä¸ªä¸“å®¶åï¼Œæ˜¯å¦å°†è¿™ K ä¸ªä¸“å®¶çš„æƒé‡é‡æ–°å½’ä¸€åŒ–ï¼ˆä½¿å…¶å’Œä¸º 1ï¼‰ã€‚æœ‰åŠ©äºæ•°å€¼ç¨³å®šã€‚
            **kwargs
    ):
        super().__init__(**kwargs)
        self.dropout = dropout
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.hidden_act = hidden_act
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.num_key_value_heads = num_key_value_heads
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.rope_theta = rope_theta
        self.inference_rope_scaling = inference_rope_scaling
        # å¤–æ¨é•¿åº¦ = factor * original_max_position_embeddings = 32768
        self.rope_scaling = {
            "beta_fast": 32,
            "beta_slow": 1,
            "factor": 16,
            "original_max_position_embeddings": 2048,
            "attention_factor": 1.0,
            "type": "yarn"
        } if self.inference_rope_scaling else None
        self.flash_attn = flash_attn
        ####################################################
        # Here are the specific configurations of MOE
        # When use_moe is false, the following is invalid
        ####################################################
        self.use_moe = use_moe
        self.num_experts_per_tok = num_experts_per_tok  # æ¯ä¸ªtokené€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = n_routed_experts  # æ€»çš„ä¸“å®¶æ•°é‡
        self.n_shared_experts = n_shared_experts  # å…±äº«ä¸“å®¶
        self.scoring_func = scoring_func  # è¯„åˆ†å‡½æ•°ï¼Œé»˜è®¤ä¸º'softmax'
        self.aux_loss_alpha = aux_loss_alpha  # è¾…åŠ©æŸå¤±çš„alphaå‚æ•°
        self.seq_aux = seq_aux  # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«ä¸Šè®¡ç®—è¾…åŠ©æŸå¤±
        self.norm_topk_prob = norm_topk_prob  # æ˜¯å¦æ ‡å‡†åŒ–top-kæ¦‚ç‡


# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MiniMind Model
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

import math
import torch
import torch.nn.init as init
import torch.nn.functional as F
from torch import nn
from transformers.activations import ACT2FN
from typing import Optional, Tuple, List, Union
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig
from transformers.modeling_outputs import CausalLMOutputWithPast


class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        # torch.rsqrt: è®¡ç®— 1/sqrtï¼Œæ¯”å…ˆ sqrt å†é™¤æ›´å¿«
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        return self.weight * self._norm(x.float()).type_as(x)


def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), rope_base: float = 1e6,
                         rope_scaling: Optional[dict] = None):
    freqs, attn_factor = 1.0 / (rope_base ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim)), 1.0
    # ========== æ­¥éª¤ 1ï¼šè®¡ç®—åŸºç¡€é¢‘ç‡ ==========
    # RoPE é¢‘ç‡å…¬å¼ï¼šf_i = 1 / (rope_base^(2i/dim))
    #   å…¶ä¸­ i æ˜¯ç»´åº¦ç´¢å¼•ï¼ˆ0, 2, 4, ..., dim-2ï¼‰ï¼Œåªä½¿ç”¨å¶æ•°ç´¢å¼•
    #   é¢‘ç‡éšç»´åº¦ç´¢å¼•å¢åŠ è€Œé€’å‡ï¼Œå½¢æˆä¸åŒé¢‘ç‡çš„æ—‹è½¬

    # ========== æ­¥éª¤ 2ï¼šåº”ç”¨ YaRN å¤–æ¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰ ==========
    if rope_scaling is not None:
        # è·å– YaRN é…ç½®å‚æ•°
        orig_max = rope_scaling.get("original_max_position_embeddings", 2048)  # è®­ç»ƒæ—¶çš„æœ€å¤§é•¿åº¦
        factor = rope_scaling.get("factor", 16)  # å¤–æ¨å› å­
        beta_fast = rope_scaling.get("beta_fast", 32.0)  # å¿«é€Ÿé¢‘ç‡è°ƒæ•´å‚æ•°
        beta_slow = rope_scaling.get("beta_slow", 1.0)  # æ…¢é€Ÿé¢‘ç‡è°ƒæ•´å‚æ•°
        attn_factor = rope_scaling.get("attention_factor", 1.0)  # æ³¨æ„åŠ›ç¼©æ”¾å› å­
        
        # å¦‚æœç›®æ ‡é•¿åº¦è¶…è¿‡è®­ç»ƒé•¿åº¦ï¼Œåº”ç”¨ YaRN å¤–æ¨
        if end / orig_max > 1.0:
            # YaRN: f'(i) = f(i)((1-Î³) + Î³/s), where Î³âˆˆ[0,1] is linear ramp
            # YaRN å…¬å¼ï¼šf'(i) = f(i) * ((1-Î³) + Î³/s)
            #   å…¶ä¸­ Î³ æ˜¯çº¿æ€§æ–œå¡å‡½æ•°ï¼Œs æ˜¯ç¼©æ”¾å› å­ï¼ˆfactorï¼‰
            #   å¯¹äºä½é¢‘ç»´åº¦ï¼ˆi < lowï¼‰ï¼Œä¸è¿›è¡Œç¼©æ”¾
            #   å¯¹äºé«˜é¢‘ç»´åº¦ï¼ˆi > highï¼‰ï¼Œå®Œå…¨ç¼©æ”¾
            #   å¯¹äºä¸­é—´ç»´åº¦ï¼Œçº¿æ€§æ’å€¼

            inv_dim = lambda b: (dim * math.log(orig_max / (b * 2 * math.pi))) / (2 * math.log(rope_base))
            # è®¡ç®—é¢‘ç‡è°ƒæ•´çš„è¾¹ç•Œç»´åº¦
            # inv_dim(b) è¿”å›é¢‘ç‡ä¸º b çš„ç»´åº¦ç´¢å¼•
            low = max(math.floor(inv_dim(beta_fast)), 0)  # ä½é¢‘è¾¹ç•Œ
            high = min(math.ceil(inv_dim(beta_slow)), dim // 2 - 1)  # é«˜é¢‘è¾¹ç•Œ

            # è®¡ç®—çº¿æ€§æ–œå¡å‡½æ•° Î³
            #   å¯¹äºç»´åº¦ iï¼šÎ³(i) = (i - low) / (high - low)ï¼Œé™åˆ¶åœ¨ [0, 1]
            ramp = torch.clamp(
                (torch.arange(dim // 2, device=freqs.device).float() - low) / max(high - low, 0.001),
                0, 1
            )
            
            # åº”ç”¨ YaRN ç¼©æ”¾ï¼šf'(i) = f(i) * ((1-Î³) + Î³/s)
            freqs = freqs * (1 - ramp + ramp / factor)

    # ========== æ­¥éª¤ 3ï¼šè®¡ç®—æ‰€æœ‰ä½ç½®çš„é¢‘ç‡ ==========
    # ä¸ºæ¯ä¸ªä½ç½®è®¡ç®—é¢‘ç‡ï¼šfreqs[pos, dim] = pos * freqs[dim]  freqs.shape = [seq_len, dim // 2]
    t = torch.arange(end, device=freqs.device) # ä½ç½®ç´¢å¼• [0, 1, 2, ..., end-1]
    freqs = torch.outer(t, freqs).float() # å¤–ç§¯ï¼š[end, dim//2]

    # ========== æ­¥éª¤ 4ï¼šè®¡ç®— cos å’Œ sin å€¼ ==========
    # å°†é¢‘ç‡è½¬æ¢ä¸º cos å’Œ sin å€¼ï¼Œç”¨äºæ—‹è½¬çŸ©é˜µ
    # ç”±äº RoPE ä½¿ç”¨å¤æ•°æ—‹è½¬ï¼Œéœ€è¦å°† dim//2 çš„é¢‘ç‡å¤åˆ¶åˆ°å®Œæ•´çš„ dim ç»´åº¦
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1) * attn_factor # [end, dim]
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1) * attn_factor # [end, dim]
    return freqs_cos, freqs_sin


def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    """
    åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰åˆ° Query å’Œ Key
    
    RoPE é€šè¿‡å¤æ•°æ—‹è½¬å°†ä½ç½®ä¿¡æ¯ç¼–ç åˆ° Q å’Œ K ä¸­ï¼š
        R_Î¸(x) = [x_0 * cos(Î¸) - x_1 * sin(Î¸), x_0 * sin(Î¸) + x_1 * cos(Î¸)] ï¼ˆäºŒç»´æ—‹è½¬çŸ©é˜µå½¢å¼ï¼‰
               = [
                    x_0 * cos(Î¸) + (- x_1) * sin(Î¸),  ï¼ˆç¬¬ 1 ç»´ï¼‰
                    x_1 * cos(Î¸) + (+ x_0) * sin(Î¸),  ï¼ˆç¬¬ 2 ç»´ï¼‰
                ]

    å…¶ä¸­ x_0 å’Œ x_1 æ˜¯è¾“å…¥å‘é‡çš„å®éƒ¨å’Œè™šéƒ¨ï¼ŒÎ¸ æ˜¯ä½ç½®ç´¢å¼•ä¸é¢‘ç‡çš„ä¹˜ç§¯ã€‚
    
    åœ¨å®ç°ä¸­ï¼Œå°†å¤æ•°æ—‹è½¬åˆ†è§£ä¸ºå®éƒ¨å’Œè™šéƒ¨çš„çº¿æ€§ç»„åˆï¼Œä½¿ç”¨ rotate_half å‡½æ•°å®ç°ã€‚
    (æ‰€è°“â€œå®éƒ¨/è™šéƒ¨â€æ˜¯æŠŠ embedding ç»´åº¦ä¸¤ä¸¤é…å¯¹å½“ä½œå¤æ•°ï¼Œå¹¶ä¸æ˜¯æ¨¡å‹å†…éƒ¨çœŸçš„å­˜å¤æ•°ã€‚)

    åˆèµ·æ¥å¯ä»¥å†™æˆä¸€ä¸ªå¾ˆå¸¸ç”¨çš„å®ç°å½¢å¼ï¼š
        rope(x) = x â‹… cosÎ¸ + rotate_half(x) â‹… sinÎ¸
    å‰ææ˜¯ rotate_half(x) å®šä¹‰ä¸º æŠŠ (x_0, x_1) å˜æˆ (-x_1, x_0) çš„å‡½æ•°ã€‚

    ---
    å®ç°ç»†èŠ‚ï¼šrotate_half çš„ä¸¤ç§å¸¸è§é…å¯¹æ–¹å¼
    
    æ–¹å¼ Aï¼šç›¸é‚»ä¸¤ç»´é…å¯¹ï¼ˆç›´è§‰ç‰ˆï¼‰
        (x_0, x_1), (x_2, x_3), ..., (x_{d-2}, x_{d-1})

    roate_half(x) æŠŠæ¯å¯¹å˜æˆ (-x_{2k+1}, x_{2k})

    æ–¹å¼ Bï¼šå‰åŠ/ååŠé…å¯¹ï¼ˆå·¥ç¨‹ç‰ˆï¼ŒLLaMA ç³»å¾ˆå¸¸è§ï¼‰
    æŠŠå‘é‡åˆ‡æˆä¸¤åŠï¼šx1=x[..., :d/2], x2=x[..., d/2:], 
    ç„¶å rotate_half(x) = concat(-x2, x1)

    è¿™ç­‰ä»·äºæŠŠç»´åº¦é…å¯¹ä¸ºï¼š
        (x_0, x_{d/2}), (x_1, x_{d/2+1}), ..., (x_{d/2-1}, x_{d-1})

    ä»ç„¶æ˜¯åŒä¸€ä¸ªäºŒç»´æ—‹è½¬ï¼Œåªæ˜¯â€œè°å’Œè°æ˜¯ä¸€å¯¹â€çš„ç´¢å¼•æ–¹å¼ä¸åŒã€‚

    ä¸ç®¡å“ªç§ï¼Œæ ¸å¿ƒæ’ç­‰å¼éƒ½ä¸€æ ·ï¼š
        x * cos + rotate_half(x) * sinã€‚
    ---

    Args:
        q: Query å¼ é‡ [batch, seq_len, num_heads, head_dim]
        k: Key å¼ é‡ [batch, seq_len, num_kv_heads, head_dim]
        cos: é¢„è®¡ç®—çš„ cos å€¼ [seq_len, head_dim]
        sin: é¢„è®¡ç®—çš„ sin å€¼ [seq_len, head_dim]
        position_ids: ä½ç½®ç´¢å¼•ï¼ˆæœªä½¿ç”¨ï¼Œcos/sin å·²åŒ…å«ä½ç½®ä¿¡æ¯ï¼‰
        unsqueeze_dim: åœ¨å“ªä¸ªç»´åº¦æ’å…¥æ–°ç»´åº¦ä»¥åŒ¹é… q/k çš„å½¢çŠ¶ï¼ˆé»˜è®¤ 1ï¼‰
        
    Returns:
        q_embed: åº”ç”¨ RoPE åçš„ Query [batch, seq_len, num_heads, head_dim]
        k_embed: åº”ç”¨ RoPE åçš„ Key [batch, seq_len, num_kv_heads, head_dim]
    """
    def rotate_half(x):
        """
        æ—‹è½¬å‘é‡çš„ååŠéƒ¨åˆ†
        
        å°†å‘é‡åˆ†æˆä¸¤åŠï¼Œäº¤æ¢ä½ç½®å¹¶å–åååŠéƒ¨åˆ†ï¼š
            [a, b, c, d] -> [-c, -d, a, b]
        
        è¿™å®ç°äº†å¤æ•°æ—‹è½¬çš„å®éƒ¨/è™šéƒ¨äº¤æ¢ã€‚
        
        Args:
            x: è¾“å…¥å¼ é‡ï¼Œæœ€åä¸€ä¸ªç»´åº¦ä¼šè¢«åˆ†æˆä¸¤åŠ
            
        Returns:
            æ—‹è½¬åçš„å¼ é‡ï¼Œå½¢çŠ¶ä¸è¾“å…¥ç›¸åŒ
        """
        # å°†æœ€åä¸€ä¸ªç»´åº¦åˆ†æˆä¸¤åŠï¼Œäº¤æ¢ä½ç½®å¹¶å–åååŠéƒ¨åˆ†
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)

    # åº”ç”¨ RoPE æ—‹è½¬
    """
    # å…¬å¼ï¼šR_Î¸(x) = x * cos(Î¸) + rotate_half(x) * sin(Î¸)
    #   è¿™ç­‰ä»·äºå¤æ•°æ—‹è½¬ï¼šx * e^(iÎ¸) = x * (cos(Î¸) + i*sin(Î¸))
    #   å…¶ä¸­ rotate_half å®ç°äº†è™šéƒ¨çš„æ“ä½œ
    """

    # è°ƒæ•´ cos å’Œ sin çš„å½¢çŠ¶ä»¥åŒ¹é… q/kï¼š[seq_len, head_dim] -> [1, seq_len, 1, head_dim]
    cos = cos.unsqueeze(unsqueeze_dim)
    sin = sin.unsqueeze(unsqueeze_dim)
    
    # å¯¹ Query å’Œ Key åˆ†åˆ«åº”ç”¨æ—‹è½¬
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed


def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """torch.repeat_interleave(x, dim=2, repeats=n_rep)"""
    """
    GQA æ˜¯ä¸€ç§æ³¨æ„åŠ›æœºåˆ¶ä¼˜åŒ–ï¼Œä½¿ç”¨è¾ƒå°‘çš„ KV heads æ¥åŒ¹é…æ›´å¤šçš„ Query headsã€‚
    ä¾‹å¦‚ï¼š8 ä¸ª Query heads å¯¹åº” 2 ä¸ª KV headsï¼Œæ¯ä¸ª KV head éœ€è¦é‡å¤ 4 æ¬¡ã€‚
    
    è¿™æ ·å¯ä»¥å‡å°‘ KV ç¼“å­˜çš„å¤§å°ï¼Œåœ¨æ¨ç†æ—¶èŠ‚çœæ˜¾å­˜ã€‚

    Args:
        x: Key æˆ– Value å¼ é‡ [batch, seq_len, num_kv_heads, head_dim]
        n_rep: æ¯ä¸ª KV head éœ€è¦é‡å¤çš„æ¬¡æ•°ï¼ˆn_rep = num_heads / num_kv_headsï¼‰
        
    Returns:
        é‡å¤åçš„å¼ é‡ [batch, seq_len, num_heads, head_dim]
    """
    bs, slen, num_key_value_heads, head_dim = x.shape
    if n_rep == 1:
        return x
    return (
        x[:, :, :, None, :] #  [B, L, num_kv_heads, 1, head_dim]
        .expand(bs, slen, num_key_value_heads, n_rep, head_dim) # [B, L, num_kv_heads, n_rep, head_dim]
        .reshape(bs, slen, num_key_value_heads * n_rep, head_dim) # [B, L, num_heads, head_dim]
    )


class Attention(nn.Module):
    def __init__(self, args: MiniMindConfig):
        super().__init__()
        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads # KV heads æ•°é‡ï¼ˆé€šå¸¸å°äº Query headsï¼‰
        assert args.num_attention_heads % self.num_key_value_heads == 0 # ç¡®ä¿ Query heads æ•°é‡èƒ½è¢« KV heads æ•°é‡æ•´é™¤

        self.n_local_heads = args.num_attention_heads # Query heads æ•°é‡, 8 ä¸ª Q å¤´
        self.n_local_kv_heads = self.num_key_value_heads # KV heads æ•°é‡, 2 ç»„ KV å¤´
        self.n_rep = self.n_local_heads // self.n_local_kv_heads # æ¯ä¸ª KV head éœ€è¦é‡å¤çš„æ¬¡æ•°, æ¯ç»„ KV å¤´è¢«å¤šå°‘ä¸ª Q å¤´å…±äº« = 4
        self.head_dim = args.hidden_size // args.num_attention_heads # æ¯ä¸ªå¤´çš„ç»´åº¦
        
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)

        self.attn_dropout = nn.Dropout(args.dropout)
        self.resid_dropout = nn.Dropout(args.dropout)
        self.dropout = args.dropout
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒ Flash Attentionï¼ˆéœ€è¦ PyTorch >= 2.0ï¼‰
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn
        # print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")

    def forward(self,
                x: torch.Tensor,
                position_embeddings: Tuple[torch.Tensor, torch.Tensor],  # ä¿®æ”¹ä¸ºæ¥æ”¶coså’Œsin
                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, # past_key_value: ç¼“å­˜çš„ KV å€¼ï¼Œç”¨äºå¢é‡è§£ç  [batch, past_len, num_kv_heads, head_dim]
                use_cache=False, # æ˜¯å¦è¿”å› KV ç¼“å­˜ä¾›ä¸‹æ¬¡ä½¿ç”¨
                attention_mask: Optional[torch.Tensor] = None # attention_mask: æ³¨æ„åŠ›æ©ç  [batch, seq_len]ï¼Œ1 è¡¨ç¤ºæœ‰æ•ˆä½ç½®ï¼Œ0 è¡¨ç¤ºæ©ç ä½ç½®
                ):
        bsz, seq_len, _ = x.shape
        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        xq = xq.view(bsz,  seq_len,  self.n_local_heads,  self.head_dim)
        xk = xk.view(bsz,  seq_len,  self.n_local_kv_heads,  self.head_dim)
        xv = xv.view(bsz,  seq_len,  self.n_local_kv_heads,  self.head_dim)

        cos, sin = position_embeddings
        xq, xk = apply_rotary_pos_emb(xq, xk, cos, sin)

        # kv_cacheå®ç°
        if past_key_value is not None:
            # å¦‚æœæœ‰ç¼“å­˜çš„ KV å€¼ï¼ˆå¢é‡è§£ç ï¼‰ï¼Œå°†å…¶ä¸å½“å‰ KV æ‹¼æ¥
            # past_key_value[0] æ˜¯ç¼“å­˜çš„ Kï¼Œpast_key_value[1] æ˜¯ç¼“å­˜çš„ V
            # åœ¨åºåˆ—ç»´åº¦ï¼ˆdim=1ï¼‰ä¸Šæ‹¼æ¥ï¼š[batch, past_len+seq_len, num_kv_heads, head_dim]
            xk = torch.cat([past_key_value[0], xk], dim=1)
            xv = torch.cat([past_key_value[1], xv], dim=1)
        
        # å¦‚æœéœ€è¦ç¼“å­˜ï¼Œä¿å­˜å½“å‰çš„ KV å€¼
        past_kv = (xk, xv) if use_cache else None

        # è°ƒæ•´ç»´åº¦é¡ºåºä¸º [batch, num_heads, seq_len, head_dim]ï¼ˆFlash Attention æ ¼å¼ï¼‰
        # å¯¹äº KVï¼Œéœ€è¦é‡å¤ heads ä»¥åŒ¹é… Query heads æ•°é‡
        xq = xq.transpose(1, 2)  # [batch, num_heads, seq_len, head_dim]
        xk = repeat_kv(xk, self.n_rep).transpose(1, 2)  # [batch, num_heads, kv_len, head_dim]
        xv = repeat_kv(xv, self.n_rep).transpose(1, 2)  # [batch, num_heads, kv_len, head_dim]

        if self.flash and (seq_len > 1) and (past_key_value is None) and (attention_mask is None or torch.all(attention_mask == 1)):
            # Flash Attentionä¸»è¦ä¼˜åŒ–çš„æ˜¯è®­ç»ƒå’Œé¢„å¡«å……ï¼ˆPrefillï¼‰é˜¶æ®µ
            # æ¡ä»¶ï¼šåºåˆ—é•¿åº¦ > 1ï¼Œä¸éœ€è¦å­˜ KV cacheï¼Œæ²¡æœ‰å¤æ‚æ©ç 
            output = F.scaled_dot_product_attention(xq, xk, xv, dropout_p=self.dropout if self.training else 0.0, is_causal=True)
        else:
            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim) # [batch, num_heads, seq_len, kv_len]
            scores[:, :, :, -seq_len:] += torch.triu(torch.full((seq_len, seq_len), float("-inf"), device=scores.device), diagonal=1) # ä¸Šä¸‰è§’çŸ©é˜µæ©ç ï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥çš„ token
            # M_causal = [[0, -inf, -inf, -inf],
            #             [0,   0, -inf, -inf],
            #             [0,   0,   0, -inf],
            #             [0,   0,   0,   0]]

            if attention_mask is not None:
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9 # 0 -> -inf, 1 -> 0
                scores = scores + extended_attention_mask

            scores = F.softmax(scores.float(), dim=-1).type_as(xq)
            scores = self.attn_dropout(scores)
            output = scores @ xv # [batch, num_heads, seq_len, head_dim]

        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)
        output = self.resid_dropout(self.o_proj(output))
        return output, past_kv


class FeedForward(nn.Module):
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        if config.intermediate_size is None:
            # æ ‡å‡†æ¯”ä¾‹ï¼šintermediate_size = hidden_size * 8/3
            #   ä¾‹å¦‚ï¼šhidden_size=512 -> intermediate_size â‰ˆ 1365
            intermediate_size = int(config.hidden_size * 8 / 3)
            # å‘ä¸Šå–æ•´åˆ° 64 çš„å€æ•°ï¼ˆä¼˜åŒ– GPU è®¡ç®—æ•ˆç‡ï¼‰
            #   ä¾‹å¦‚ï¼š1365 -> 1408 (64 * 22)
            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)

        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)
        self.dropout = nn.Dropout(config.dropout)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)))


class MoEGate(nn.Module):
    """
    MoE (Mixture of Experts) é—¨æ§ç½‘ç»œ
    
    è´Ÿè´£ä¸ºæ¯ä¸ª token é€‰æ‹© top-k ä¸ªä¸“å®¶ï¼Œå¹¶è®¡ç®—ä¸“å®¶æƒé‡ã€‚
    ä½¿ç”¨è¾…åŠ©æŸå¤±ï¼ˆauxiliary lossï¼‰æ¥é¼“åŠ±ä¸“å®¶è´Ÿè½½å‡è¡¡ï¼Œé˜²æ­¢ä¸“å®¶é€€åŒ–ã€‚
    
    å·¥ä½œæµç¨‹ï¼š
        1. è®¡ç®—æ¯ä¸ªä¸“å®¶å¯¹æ¯ä¸ª token çš„åˆ†æ•°ï¼ˆlogitsï¼‰
        2. ä½¿ç”¨ softmax è½¬æ¢ä¸ºæ¦‚ç‡
        3. é€‰æ‹© top-k ä¸ªä¸“å®¶
        4. è®¡ç®—è¾…åŠ©æŸå¤±ï¼ˆè®­ç»ƒæ—¶ï¼‰
    """
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.top_k = config.num_experts_per_tok # æ¯ä¸ª token é€‰æ‹©çš„ä¸“å®¶æ•°é‡
        self.n_routed_experts = config.n_routed_experts # ä¸“å®¶æ€»æ•°

        self.scoring_func = config.scoring_func # è¯„åˆ†å‡½æ•°ï¼ˆ'softmax'ï¼‰
        self.alpha = config.aux_loss_alpha # è¾…åŠ©æŸå¤±æƒé‡
        self.seq_aux = config.seq_aux # æ˜¯å¦åœ¨åºåˆ—çº§åˆ«è®¡ç®—è¾…åŠ©æŸå¤±

        self.norm_topk_prob = config.norm_topk_prob # æ˜¯å¦æ ‡å‡†åŒ– top-k æ¦‚ç‡
        self.gating_dim = config.hidden_size # é—¨æ§ç½‘ç»œè¾“å…¥ç»´åº¦
        
        self.weight = nn.Parameter(torch.empty((self.n_routed_experts, self.gating_dim)))
        # é—¨æ§ç½‘ç»œæƒé‡ï¼š[n_routed_experts, hidden_size]
        #   æ¯ä¸€è¡Œå¯¹åº”ä¸€ä¸ªä¸“å®¶çš„æƒé‡å‘é‡
        self.reset_parameters()

    def reset_parameters(self) -> None:
        init.kaiming_uniform_(self.weight, a=math.sqrt(5))

    def forward(self, hidden_states):
        """
        å‰å‘ä¼ æ’­ï¼šä¸ºæ¯ä¸ª token é€‰æ‹©ä¸“å®¶
        
        Args:
            hidden_states: è¾“å…¥å¼ é‡ [batch, seq_len, hidden_size]
            
        Returns:
            topk_idx: é€‰æ‹©çš„ä¸“å®¶ç´¢å¼• [batch * seq_len, top_k]
            topk_weight: ä¸“å®¶æƒé‡ [batch * seq_len, top_k]
            aux_loss: è¾…åŠ©æŸå¤±ï¼ˆæ ‡é‡ï¼‰ï¼Œç”¨äºé¼“åŠ±è´Ÿè½½å‡è¡¡
        """

        # hidden_states: è¾“å…¥æ•°æ®ã€‚
        # å½¢çŠ¶æ˜¯ [batch(æ‰¹æ¬¡å¤§å°), seq_len(å¥å­é•¿åº¦), h(éšè—å±‚ç»´åº¦)]
        # ä¾‹å¦‚: [2, 10, 512] è¡¨ç¤º 2 å¥è¯ï¼Œæ¯å¥ 10 ä¸ªè¯ï¼Œæ¯ä¸ªè¯ç”¨ 512 ç»´å‘é‡è¡¨ç¤ºã€‚
        bsz, seq_len, h = hidden_states.shape
        
        # ========== æ­¥éª¤ 1ï¼šè®¡ç®—ä¸“å®¶åˆ†æ•° ==========
            
        # .view(-1, h): ç»“æœå½¢çŠ¶å˜ä¸º [batch * seq_len, h]ã€‚
        # å«ä¹‰ï¼šæŠŠæ‰€æœ‰å¥å­çš„æ‰€æœ‰è¯å¹³é“ºå¼€ï¼Œå˜æˆä¸€ä¸ªé•¿é•¿çš„åˆ—è¡¨ï¼Œå› ä¸ºæˆ‘ä»¬å¯¹æ¯ä¸ªè¯æ˜¯ç‹¬ç«‹å¤„ç†çš„ã€‚
        hidden_states = hidden_states.view(-1, h)

        # F.linear(input, weight): çº¿æ€§å±‚è®¡ç®—ï¼Œæ•°å­¦å…¬å¼æ˜¯ Y = XW^Tã€‚
        # hidden_states å½¢çŠ¶ [Total_Tokens, h]
        # self.weight å½¢çŠ¶ [n_routed_experts, h]
        # ç»“æœ logits å½¢çŠ¶ [Total_Tokens, n_routed_experts]
        # å«ä¹‰ï¼šè®¡ç®—æ¯ä¸ª Token å’Œæ¯ä¸ª Expert çš„åŒ¹é…åˆ†æ•°ï¼ˆåŸå§‹åˆ†æ•°ï¼Œæœªå½’ä¸€åŒ–ï¼‰ã€‚
        logits = F.linear(hidden_states, self.weight, None)

        # ========== æ­¥éª¤ 2ï¼šè½¬æ¢ä¸ºæ¦‚ç‡ ==========
        if self.scoring_func == 'softmax':
            scores = logits.softmax(dim=-1) # [batch * seq_len, n_routed_experts]
        else:
            raise NotImplementedError(f'insupportable scoring function for MoE gating: {self.scoring_func}')

        # ========== æ­¥éª¤ 3ï¼šé€‰æ‹© top-k ä¸“å®¶ ==========
        # torch.topk: å¯»æ‰¾å¼ é‡ä¸­æœ€å¤§çš„ k ä¸ªå€¼ã€‚
        # scores: æ¥æºå¼ é‡ã€‚
        # k=self.top_k: è¦é€‰å‡ ä¸ªï¼ˆæ¯”å¦‚ 2 ä¸ªï¼‰ã€‚
        # dim=-1: åœ¨ä¸“å®¶ç»´åº¦ä¸Šé€‰ã€‚
        # sorted=False: ä¸éœ€è¦å¯¹é€‰å‡ºæ¥çš„ç»“æœæ’åºï¼ˆä¸ºäº†é€Ÿåº¦ï¼‰ã€‚
        # è¿”å›å€¼ï¼š
        #   topk_weight: [batch * seq_len, top_k] é€‰ä¸­çš„é‚£ k ä¸ªä¸“å®¶çš„æ¦‚ç‡å€¼ã€‚
        #   topk_idx: [batch * seq_len, top_k] é€‰ä¸­çš„é‚£ k ä¸ªä¸“å®¶çš„ç´¢å¼•ï¼ˆID å·ï¼‰ã€‚
        topk_weight, topk_idx = torch.topk(scores, k=self.top_k, dim=-1, sorted=False)

        # ========== æ­¥éª¤ 4ï¼šæ ‡å‡†åŒ– top-k æ¦‚ç‡ï¼ˆå¯é€‰ï¼‰ ==========
        if self.top_k > 1 and self.norm_topk_prob:
            # å°† top-k ä¸ªä¸“å®¶çš„æƒé‡æ ‡å‡†åŒ–ï¼Œä½¿å…¶å’Œä¸º 1
            #   è¿™æ ·ç¡®ä¿æ¯ä¸ª token çš„ä¸“å®¶æƒé‡åˆ†å¸ƒæ˜¯å½’ä¸€åŒ–çš„
            denominator = topk_weight.sum(dim=-1, keepdim=True) + 1e-20
            topk_weight = topk_weight / denominator

        if self.training and self.alpha > 0.0:
            # ========== æ­¥éª¤ 5ï¼šè®¡ç®—è¾…åŠ©æŸå¤±ï¼ˆè®­ç»ƒæ—¶ï¼‰ ==========
            # è¾…åŠ©æŸå¤±ç”¨äºé¼“åŠ±ä¸“å®¶è´Ÿè½½å‡è¡¡ï¼Œé˜²æ­¢æŸäº›ä¸“å®¶è¢«è¿‡åº¦ä½¿ç”¨æˆ–å®Œå…¨ä¸ç”¨

            scores_for_aux = scores # ä¹Ÿå°±æ˜¯æ‰€æœ‰ä¸“å®¶åŸæœ¬çš„æ¦‚ç‡åˆ†å¸ƒ
            aux_topk = self.top_k
            topk_idx_for_aux_loss = topk_idx.view(bsz, -1) # [batch, seq_len * top_k]

            if self.seq_aux:
                # === æ–¹æ¡ˆ Aï¼šåºåˆ—çº§è¾…åŠ©æŸå¤± (DeepSeek-V2/V3 å¸¸ç”¨) ===
                # è¿™ç§è®¡ç®—æ–¹å¼æ›´ç²¾ç»†ï¼Œåœ¨æ¯æ¡æ ·æœ¬å†…éƒ¨çœ‹è´Ÿè½½å‡è¡¡ã€‚
                
                # å˜å½¢å› [batch, seq_len, n_routed_experts]
                scores_for_seq_aux = scores_for_aux.view(bsz, seq_len, -1)

                # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„ä½¿ç”¨é¢‘ç‡ï¼ˆæœŸæœ›è´Ÿè½½ï¼‰
                # åˆ›å»ºä¸€ä¸ªå…¨ 0 çŸ©é˜µç”¨æ¥ç»Ÿè®¡æ¬¡æ•°
                ce = torch.zeros(bsz, self.n_routed_experts, device=hidden_states.device)
                
                # scatter_add_: è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„â€œæ•£å°„åŠ æ³•â€æ“ä½œã€‚
                # å½¢è±¡ç†è§£ï¼šè¿™æ˜¯åœ¨â€œæŠ•ç¥¨â€ã€‚
                # topk_idx_for_aux_loss é‡Œçš„å€¼æ˜¯ä¸“å®¶ IDï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬æ¯ä¸ª Token æŠ•ç»™äº†è°ã€‚
                # è¿™è¡Œä»£ç ç»Ÿè®¡ï¼šåœ¨è¿™ä¸ª Batch é‡Œï¼Œæ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­äº†å¤šå°‘æ¬¡ã€‚
                ce.scatter_add_(
                    1, # dim
                    topk_idx_for_aux_loss,
                    torch.ones(bsz, seq_len * aux_topk, device=hidden_states.device) # [batch, seq_len * top_k]
                ).div_(seq_len * aux_topk / self.n_routed_experts)
                # .div_(...): é™¤ä»¥æœŸæœ›çš„å¹³å‡æ¬¡æ•°ï¼Œå°†å…¶å½’ä¸€åŒ–ã€‚
                # å¦‚æœ ce = 1ï¼Œè¯´æ˜è¯¥ä¸“å®¶è¢«é€‰ä¸­çš„é¢‘ç‡æ­£å¥½ç­‰äºå¹³å‡æ°´å¹³ã€‚

                # è®¡ç®—æŸå¤±ï¼š(å®é™…ä½¿ç”¨é¢‘ç‡ * ä¸“å®¶å¹³å‡æ¦‚ç‡å¾—åˆ†)
                # è¿™ç§æŸå¤±è®¾è®¡ä¼šè¿«ä½¿æ¨¡å‹å€¾å‘äºè®©æ‰€æœ‰ä¸“å®¶çš„ä½¿ç”¨é¢‘ç‡å’Œå¹³å‡å¾—åˆ†è¶‹äºä¸€è‡´ã€‚
                aux_loss = (ce * scores_for_seq_aux.mean(dim=1)).sum(dim=1).mean() * self.alpha
            else:
                # === æ–¹æ¡ˆ Bï¼šToken çº§è¾…åŠ©æŸå¤± (ä¼ ç»Ÿçš„ Switch Transformer åšæ³•) ===
                # è¿™ç§æ˜¯å…¨å±€ç»Ÿè®¡æ‰€æœ‰ Tokenã€‚
                
                # F.one_hot: ç‹¬çƒ­ç¼–ç ã€‚å¦‚æœ ID æ˜¯ 3ï¼Œå˜æˆ [0, 0, 0, 1, 0...]
                mask_ce = F.one_hot(topk_idx_for_aux_loss.view(-1), num_classes=self.n_routed_experts)
                # mask_ce: [batch * seq_len * top_k, n_routed_experts]
                # [
                #   [0, 0, 1, 0, 0, ..., 0 (n_routed_experts-1 åˆ—)], # ç¬¬ä¸€ä¸ª token é€‰äº†ç¬¬ 3 ä¸ªä¸“å®¶
                #   [0, 1, 0, 0, 0, ..., 0 (n_routed_experts-1 åˆ—)], # ç¬¬äºŒä¸ª token é€‰äº†ç¬¬ 2 ä¸ªä¸“å®¶
                #   ...
                #   [0, 0, 0, 1, 0, ..., 0 (n_routed_experts-1 åˆ—)], # ç¬¬ N ä¸ª token é€‰äº†ç¬¬ 4 ä¸ªä¸“å®¶
                # ]

                ce = mask_ce.float().mean(dim=0) # [n_routed_experts] - ç»Ÿè®¡æ¯ä¸ªä¸“å®¶çš„å¹³å‡ä½¿ç”¨é¢‘ç‡

                # è®¡ç®—æ¯ä¸ªä¸“å®¶å¾—åˆ°çš„å¹³å‡åˆ†ï¼ˆæ¨¡å‹â€œæƒ³â€é€‰å®ƒçš„ç¨‹åº¦ï¼‰ã€‚
                Pi = scores_for_aux.mean(dim=0) # [n_routed_experts] - æ¯ä¸ªä¸“å®¶çš„å¹³å‡åˆ†æ•°

                # è®¡ç®—è´Ÿè½½å‡è¡¡åˆ†æ•°
                fi = ce * self.n_routed_experts

                # ç»å…¸çš„è´Ÿè½½å‡è¡¡æŸå¤±å…¬å¼ï¼š
                # minimize (N * sum(Pi * fi))
                # åªæœ‰å½“æ¦‚ç‡åˆ†å¸ƒæ˜¯å‡åŒ€åˆ†å¸ƒæ—¶ï¼Œè¿™ä¸ªç‚¹ç§¯æœ€å°ã€‚
                aux_loss = (Pi * fi).sum() * self.alpha
        else:
            # å¦‚æœä¸åœ¨è®­ç»ƒï¼Œæˆ–è€…ä¸éœ€è¦è¾…åŠ©æŸå¤±ï¼ŒæŸå¤±ä¸º 0
            aux_loss = scores.new_zeros(1).squeeze()

        return topk_idx, topk_weight, aux_loss


class MOEFeedForward(nn.Module):
    """
    MoE (Mixture of Experts) å‰é¦ˆç½‘ç»œ
    
    ä½¿ç”¨å¤šä¸ªä¸“å®¶ï¼ˆFeedForwardï¼‰å¤„ç†ä¸åŒçš„ tokenï¼Œé€šè¿‡é—¨æ§ç½‘ç»œåŠ¨æ€é€‰æ‹©ä¸“å®¶ã€‚
    æ”¯æŒ è·¯ç”±ä¸“å®¶ï¼ˆrouted expertsï¼‰å’Œ å…±äº«ä¸“å®¶ï¼ˆshared expertsï¼‰ä¸¤ç§ç±»å‹ã€‚
    
    å·¥ä½œæµç¨‹ï¼š
        1. é—¨æ§ç½‘ç»œä¸ºæ¯ä¸ª token é€‰æ‹© top-k ä¸ªè·¯ç”±ä¸“å®¶
        2. æ¯ä¸ª token è¢«è·¯ç”±åˆ°é€‰ä¸­çš„ä¸“å®¶å¤„ç†
        3. ä¸“å®¶è¾“å‡ºæŒ‰æƒé‡åŠ æƒæ±‚å’Œ
        4. å…±äº«ä¸“å®¶å¤„ç†æ‰€æœ‰ token å¹¶æ·»åŠ åˆ°è¾“å‡º

    Output = å…±äº«ä¸“å®¶è¾“å‡º + Î£(è·¯ç”±ä¸“å®¶è¾“å‡º * é—¨æ§æƒé‡)
    """
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config

        # è·¯ç”±ä¸“å®¶ï¼šé€šè¿‡é—¨æ§ç½‘ç»œåŠ¨æ€é€‰æ‹©ï¼Œæ¯ä¸ª token åªä½¿ç”¨ top-k ä¸ªä¸“å®¶
        self.experts = nn.ModuleList([
            FeedForward(config)
            for _ in range(config.n_routed_experts)
        ])

        # è´Ÿè´£ä¸ºæ¯ä¸ª token é€‰æ‹©ä¸“å®¶å¹¶è®¡ç®—æƒé‡
        self.gate = MoEGate(config)

        # å…±äº«ä¸“å®¶ï¼šå¤„ç†æ‰€æœ‰ tokenï¼Œä¸ç»è¿‡é—¨æ§ç½‘ç»œ
        #   ç”¨äºæä¾›é€šç”¨ç‰¹å¾ï¼Œå¢å¼ºæ¨¡å‹è¡¨è¾¾èƒ½åŠ›
        if config.n_shared_experts > 0:
            self.shared_experts = nn.ModuleList([
                FeedForward(config)
                for _ in range(config.n_shared_experts)
            ])

    def forward(self, x):
        """
        Args:
            x: è¾“å…¥å¼ é‡ [batch, seq_len, hidden_size]
            
        Returns:
            è¾“å‡ºå¼ é‡ [batch, seq_len, hidden_size]
        """
        identity = x  # ä¿å­˜åŸå§‹è¾“å…¥ï¼Œç”¨äºå…±äº«ä¸“å®¶
        orig_shape = x.shape
        bsz, seq_len, _ = x.shape
        
         # ========== æ­¥éª¤ 1ï¼šé—¨æ§ç½‘ç»œé€‰æ‹©ä¸“å®¶ ==========
        # ä¸ºæ¯ä¸ª token é€‰æ‹© top-k ä¸ªä¸“å®¶å¹¶è®¡ç®—æƒé‡
        topk_idx, topk_weight, aux_loss = self.gate(x)
        # topk_idx: [batch * seq_len, top_k] - ä¸“å®¶ç´¢å¼•
        # topk_weight: [batch * seq_len, top_k] - ä¸“å®¶æƒé‡

        # ========== æ­¥éª¤ 2ï¼šè·¯ç”±åˆ°ä¸“å®¶å¤„ç† ==========
        x = x.view(-1, x.shape[-1]) # [batch * seq_len, hidden_size]
        flat_topk_idx = topk_idx.view(-1) # [batch * seq_len * top_k] - å±•å¹³çš„ä¸“å®¶ç´¢å¼•
        
        if self.training:
            # è®­ç»ƒæ¨¡å¼ï¼š
            """ç›®æ ‡ï¼šå¿…é¡»æ„å»ºä¸€å¼ å®Œæ•´çš„ã€æ­£ç¡®çš„è®¡ç®—å›¾ (Computational Graph)ï¼Œä»¥ä¾¿æ¢¯åº¦ï¼ˆGradientsï¼‰èƒ½å¤Ÿåå‘ä¼ æ’­æ›´æ–°å‚æ•°ã€‚"""
            # num_experts_per_tok: Top-K è·¯ç”±æ•°ã€‚æ¯ä¸ª Token åœ¨æ¨ç†æ—¶å®é™…ä¼šæ¿€æ´»çš„è·¯ç”±ä¸“å®¶æ•°é‡ã€‚
            # ä¸ºæ¯ä¸ª token çš„æ¯ä¸ªé€‰ä¸­ä¸“å®¶å¤åˆ¶è¾“å…¥
            #   ä¾‹å¦‚ï¼štop_k=2ï¼Œæ¯ä¸ª token éœ€è¦å¤„ç† 2 æ¬¡
            x = x.repeat_interleave(self.config.num_experts_per_tok, dim=0) # [batch * seq_len * top_k, hidden_size]
            """
            å½“ top_k > 1 æ—¶ï¼ˆä¾‹å¦‚æ¯ä¸ª token é€‰ 2 ä¸ªä¸“å®¶ï¼‰ï¼ŒåŒä¸€ä¸ª token çš„å‘é‡éœ€è¦è¢«é€å…¥ä¸¤ä¸ªä¸åŒçš„ä¸“å®¶ã€‚
            åœ¨è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ repeat_interleave æ˜¾å¼åœ°æŠŠæ•°æ®å¤åˆ¶ä¸€ä»½ã€‚
            ä¸ºä»€ä¹ˆï¼Ÿ è¿™æ ·åšå¯ä»¥è®© PyTorch æ¸…æ¥šåœ°çŸ¥é“ï¼šä¸“å®¶ A çš„æ¢¯åº¦è¦ä¼ å›ç»™å‰¯æœ¬ 1ï¼Œä¸“å®¶ B çš„æ¢¯åº¦è¦ä¼ å›ç»™å‰¯æœ¬ 2ï¼Œæœ€ååœ¨åº•å±‚è¿™ä¸¤ä¸ªæ¢¯åº¦ä¼šè‡ªåŠ¨åŠ å’Œï¼ˆAccumulateï¼‰å›åŸå§‹çš„ token embeddingã€‚
            """

            # è¾“å…¥æ˜¯ä¸€ä¸ªå¤§çŸ©é˜µï¼Œä½†æˆ‘ä»¬éœ€è¦æŠŠå®ƒæ‹†å¾—æ”¯ç¦»ç ´ç¢ï¼Œé€è¿›ä¸åŒçš„ä¸“å®¶ï¼Œç®—å®Œå†æ‹¼å›æ¥ã€‚
            # ç»“æœèšåˆ, æ˜¾å¼ç´¢å¼•èµ‹å€¼
            y = torch.empty_like(x, dtype=x.dtype) # [batch * seq_len * top_k, hidden_size]

            # å¯¹æ¯ä¸ªä¸“å®¶ï¼Œå¤„ç†åˆ†é…ç»™å®ƒçš„ token
            for i, expert in enumerate(self.experts):
                # æ‰¾åˆ°åˆ†é…ç»™ä¸“å®¶ i çš„ token ç´¢å¼•ï¼Œå¹¶å¤„ç†è¿™äº› token
                expert_out = expert(x[flat_topk_idx == i])

                if expert_out.shape[0] > 0:
                    # å¦‚æœæœ‰ token åˆ†é…ç»™è¯¥ä¸“å®¶ï¼Œä¿å­˜è¾“å‡º
                    y[flat_topk_idx == i] = expert_out.to(y.dtype)
                else:
                    # å¦‚æœæ²¡æœ‰ token åˆ†é…ç»™è¯¥ä¸“å®¶ï¼Œä¹Ÿéœ€è¦åˆ›å»ºç©ºè¾“å‡ºï¼ˆä¿æŒæ¢¯åº¦æµï¼‰å¦åˆ™ä¼šå¯¼è‡´ DDP å¡æ­»ï¼ˆè·Ÿæ¨ç†æ—¶ä¸ä¸€æ ·ï¼‰
                    y[flat_topk_idx == i] = expert_out.to(y.dtype) + 0 * sum(p.sum() for p in expert.parameters())
                    """
                    åŸå› ï¼šåœ¨ä½¿ç”¨å¤šå¡åˆ†å¸ƒå¼è®­ç»ƒï¼ˆDDPï¼‰æ—¶ï¼Œå¦‚æœæŸä¸ªä¸“å®¶åœ¨æŸå¼ å¡ä¸Šæ°å¥½æ²¡æœ‰åˆ†é…åˆ°ä»»ä½•æ•°æ®ï¼ˆflat_topk_idx == i å…¨ä¸º Falseï¼‰ï¼Œå®ƒçš„æ¢¯åº¦å°±æ˜¯ Noneã€‚è¿™ä¼šå¯¼è‡´ DDP åœ¨è¿›ç¨‹åŒæ­¥æ—¶å¡æ­»ï¼ˆHangï¼‰ã€‚
                    è§£å†³ï¼šè¿™è¡Œä»£ç å¼ºè¡Œæ„é€ äº†ä¸€ä¸ªâ€œå€¼ä¸º 0 ä½†ä¾èµ–äºä¸“å®¶å‚æ•°â€çš„è®¡ç®—èŠ‚ç‚¹ï¼Œç¡®ä¿æ¢¯åº¦æµä¸æ–­ï¼Œé˜²æ­¢è®­ç»ƒå¡æ­»ã€‚æ¨ç†æ—¶ä¸éœ€è¦åå‘ä¼ æ’­ï¼Œè‡ªç„¶ä¸éœ€è¦è¿™ä¸ª hackã€‚
                    """

            # æŒ‰æƒé‡åŠ æƒæ±‚å’Œï¼šæ¯ä¸ª token çš„ top-k ä¸ªä¸“å®¶è¾“å‡ºåŠ æƒå¹³å‡
            y = (
                y.view(*topk_weight.shape, -1) # [batch * seq_len, top_k, hidden_size]
                * topk_weight.unsqueeze(-1) # [batch * seq_len, top_k, 1]
            ).sum(dim=1) # [batch * seq_len, hidden_size]
            y = y.view(*orig_shape) # [batch, seq_len, hidden_size]

        else:
            # æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨ä¼˜åŒ–çš„æ¨ç†å‡½æ•°
            """ç›®æ ‡ï¼šä¸éœ€è¦ç®—æ¢¯åº¦ï¼Œåªéœ€è¦ä»¥å‰å‘ä¼ æ’­æœ€å¿«çš„æ–¹å¼å¾—åˆ° Y"""
            y = self.moe_infer(x, flat_topk_idx, topk_weight.view(-1, 1)).view(*orig_shape)

        # ========== æ­¥éª¤ 3ï¼šæ·»åŠ å…±äº«ä¸“å®¶è¾“å‡º ==========
        # å…±äº«ä¸“å®¶å¤„ç†æ‰€æœ‰ tokenï¼Œè¾“å‡ºç›´æ¥æ·»åŠ åˆ°ç»“æœä¸­
        if self.config.n_shared_experts > 0:
            for expert in self.shared_experts:
                y = y + expert(identity) # æ®‹å·®è¿æ¥
        
        # ä¿å­˜è¾…åŠ©æŸå¤±ä¾›åç»­ä½¿ç”¨
        self.aux_loss = aux_loss
        return y

    @torch.no_grad()
    def moe_infer(self, x, flat_expert_indices, flat_expert_weights):
        """
        ä¼˜åŒ–çš„ MoE æ¨ç†å‡½æ•°ï¼ˆä»…æ¨ç†æ—¶ä½¿ç”¨ï¼‰ 
        
        é€šè¿‡æ‰¹é‡å¤„ç†æ¯ä¸ªä¸“å®¶çš„æ‰€æœ‰ tokenï¼Œå‡å°‘è®¡ç®—å¼€é”€ã€‚
        å·¥ä½œæµç¨‹ï¼š
            1. æŒ‰ä¸“å®¶ç´¢å¼•æ’åº token
            2. ç»Ÿè®¡æ¯ä¸ªä¸“å®¶å¤„ç†çš„ token æ•°é‡
            3. æ‰¹é‡å¤„ç†æ¯ä¸ªä¸“å®¶çš„æ‰€æœ‰ token
            4. æŒ‰æƒé‡åŠ æƒå¹¶ç´¯åŠ åˆ°è¾“å‡ºç¼“å­˜
        
        Args:
            x: è¾“å…¥å¼ é‡ [batch * seq_len, hidden_size]
            flat_expert_indices: å±•å¹³çš„ä¸“å®¶ç´¢å¼• [batch * seq_len * top_k]
            flat_expert_weights: å±•å¹³çš„ä¸“å®¶æƒé‡ [batch * seq_len * top_k, 1]
            
        Returns:
            è¾“å‡ºå¼ é‡ [batch * seq_len, hidden_size]
        """
        expert_cache = torch.zeros_like(x) # è¾“å‡ºç¼“å­˜

        # ========== æ­¥éª¤ 1ï¼šæŒ‰ä¸“å®¶ç´¢å¼•æ’åº ==========
        # å°† token æŒ‰ä¸“å®¶ç´¢å¼•æ’åºï¼Œä½¿åŒä¸€ä¸“å®¶çš„ token èšé›†åœ¨ä¸€èµ·
        idxs = flat_expert_indices.argsort()  # æ’åºåçš„ç´¢å¼•

        # ========== æ­¥éª¤ 2ï¼šç»Ÿè®¡æ¯ä¸ªä¸“å®¶å¤„ç†çš„ token æ•°é‡ ==========
        # bincount: ç»Ÿè®¡æ¯ä¸ªä¸“å®¶è¢«é€‰ä¸­çš„æ¬¡æ•°
        # cumsum: ç´¯ç§¯å’Œï¼Œå¾—åˆ°æ¯ä¸ªä¸“å®¶çš„ token èŒƒå›´
        # ä¸€æ¬¡æ€§ç®—å‡ºæ¯ä¸ªä¸“å®¶å¤„ç†å¤šå°‘æ•°æ®ï¼Œä»¥åŠæ•°æ®åœ¨æ•°ç»„ä¸­çš„èµ·æ­¢ä½ç½®ã€‚
        #   ä¾‹å¦‚ï¼š[6, 15, 20, 26] è¡¨ç¤ºï¼š
        #     - ä¸“å®¶ 0 å¤„ç†å‰ 6 ä¸ª token
        #     - ä¸“å®¶ 1 å¤„ç†ç¬¬ 6-15 ä¸ª token
        #     - ä¸“å®¶ 2 å¤„ç†ç¬¬ 15-20 ä¸ª token
        #     - ä¸“å®¶ 3 å¤„ç†ç¬¬ 20-26 ä¸ª token
        tokens_per_expert = flat_expert_indices.bincount().cpu().numpy().cumsum(0)
        
        # è®¡ç®—æ¯ä¸ª token çš„åŸå§‹ç´¢å¼•ï¼ˆå»é™¤ top_k çš„é‡å¤ï¼‰
        token_idxs = idxs // self.config.num_experts_per_tok

        # ========== æ­¥éª¤ 3ï¼šæ‰¹é‡å¤„ç†æ¯ä¸ªä¸“å®¶ ==========
        # å½“ tokens_per_expert = [6, 15, 20, 26]ï¼Œtokens_per_expert.shape[0]å³ä¸ºä¸“å®¶æ•°é‡ï¼ˆæ­¤æ—¶ä¸º4ï¼‰
        # ä¸” token_idxs = [3, 7, 19, 21, 24, 25,  4,  5,  6, 10, 11, 12...] æ—¶
        # æ„å‘³ token_idxs[:6] -> [3, 7, 19, 21, 24, 25]è¿™6ä¸ªä½ç½®å±äºä¸“å®¶0å¤„ç†çš„tokenï¼ˆæ¯ä¸ªtokenæœ‰å¯èƒ½è¢«å¤šä¸ªä¸“å®¶å¤„ç†ï¼Œè¿™å–å†³äºnum_experts_per_tokï¼‰
        # æ¥ä¸‹æ¥9ä¸ªä½ç½® token_idxs[6:15] -> [4,  5,  6, 10, 11, 12...]å±äºä¸“å®¶1å¤„ç†çš„token...ä¾æ­¤ç±»æ¨
        for i, end_idx in enumerate(tokens_per_expert):
            start_idx = 0 if i == 0 else tokens_per_expert[i - 1]

            # å¦‚æœè¯¥ä¸“å®¶æ²¡æœ‰å¤„ç†çš„ tokenï¼Œè·³è¿‡ è¾¾åˆ°æ¨ç†åŠ é€Ÿçš„ç›®çš„
            if start_idx == end_idx:
                continue

            expert = self.experts[i]
            exp_token_idx = token_idxs[start_idx:end_idx]
            expert_tokens = x[exp_token_idx] # è¯¥ä¸“å®¶éœ€è¦å¤„ç†çš„ token

            # æ‰¹é‡å¤„ç†è¯¥ä¸“å®¶çš„æ‰€æœ‰ token
            expert_out = expert(expert_tokens).to(expert_cache.dtype)
            # åº”ç”¨æƒé‡
            expert_out.mul_(flat_expert_weights[idxs[start_idx:end_idx]])
            # ç´¯åŠ åˆ°è¾“å‡ºç¼“å­˜ï¼ˆä½¿ç”¨ scatter_add å¤„ç†åŒä¸€ token è¢«å¤šä¸ªä¸“å®¶å¤„ç†çš„æƒ…å†µï¼‰
            expert_cache.scatter_add_(
                0,
                exp_token_idx.view(-1, 1).repeat(1, x.shape[-1]),
                expert_out
            )
            # åŸå­æ“ä½œï¼Œç›´æ¥æŠŠç»“æœâ€œç´¯åŠ â€åˆ°è¾“å‡ºç¼“å†²åŒºå¯¹åº”çš„ä½ç½®

        return expert_cache


class MiniMindBlock(nn.Module):
    def __init__(self, layer_id: int, config: MiniMindConfig):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads
        self.hidden_size = config.hidden_size
        self.head_dim = config.hidden_size // config.num_attention_heads
        self.self_attn = Attention(config)

        self.layer_id = layer_id
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.mlp = FeedForward(config) if not config.use_moe else MOEFeedForward(config)

    def forward(self, hidden_states, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):
        residual = hidden_states
        hidden_states, present_key_value = self.self_attn(
            self.input_layernorm(hidden_states), position_embeddings,
            past_key_value, use_cache, attention_mask
        )
        hidden_states += residual
        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states))
        return hidden_states, present_key_value


class MiniMindModel(nn.Module):
    # Input IDs -> Embeddings -> [Transformer Blocks x L] -> RMSNorm -> Output Hidden States
    def __init__(self, config: MiniMindConfig):
        super().__init__()
        self.config = config
        self.vocab_size, self.num_hidden_layers = config.vocab_size, config.num_hidden_layers

        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size)
        self.dropout = nn.Dropout(config.dropout)
        self.layers = nn.ModuleList([MiniMindBlock(l, config) for l in range(self.num_hidden_layers)])
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        # åœ¨è¾“å‡ºä¹‹å‰è¿›è¡Œæœ€åä¸€æ¬¡ RMSNormï¼Œè¿™æ˜¯ LLaMA æ¶æ„çš„æ ‡å‡†åšæ³•
        # å½¢çŠ¶: [H]

        # é¢„å…ˆè®¡ç®—æ‰€æœ‰å¯èƒ½ä½ç½®çš„ Cos å’Œ Sin å€¼ï¼Œé¿å…å‰å‘ä¼ æ’­æ—¶é‡å¤è®¡ç®—
        # freqs_cos/sin å½¢çŠ¶: [MaxPos, HD]
        freqs_cos, freqs_sin = precompute_freqs_cis(
            dim=config.hidden_size // config.num_attention_heads, # Head Dim
            end=config.max_position_embeddings, # æœ€å¤§ä½ç½®ç´¢å¼• (å¦‚ 32768)
            rope_base=config.rope_theta, # RoPE åŸºé¢‘
            rope_scaling=config.rope_scaling
        )
        # å°†é¢‘ç‡è¡¨æ³¨å†Œä¸º buffer
        # buffer ä¸ä¼šè¢«è§†ä¸ºæ¨¡å‹å‚æ•° (parameter)ï¼Œä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼Œä½†ä¼šéšæ¨¡å‹æƒé‡æ–‡ä»¶ä¿å­˜
        # persistent=False è¡¨ç¤ºè¿™äº›å€¼å¯ä»¥æ ¹æ® config åŠ¨æ€é‡æ–°è®¡ç®—ï¼Œä¸å¼ºåˆ¶ä¾èµ–æƒé‡æ–‡ä»¶
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                **kwargs):
        """
        Args:
            input_ids: è¾“å…¥åºåˆ— [B, S]ã€‚
                       è®­ç»ƒæ—¶ S æ˜¯æ•´ä¸ªå¥å­é•¿åº¦ï¼›
                       æ¨ç† Decoding é˜¶æ®µ S é€šå¸¸ä¸º 1ã€‚
            attention_mask: æ©ç  [B, S]ã€‚
            past_key_values: å†å² KV ç¼“å­˜åˆ—è¡¨ï¼Œç¼“å­˜æ¯ä¸€å±‚ layer çš„ KVã€‚
                             List é•¿åº¦ä¸º Lï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (K, V) å…ƒç»„ï¼Œé•¿åº¦ç­‰äº layer æ•°é‡ã€‚
                             K/V å½¢çŠ¶: [B, Past_Len, Num_KV_Heads, HD]ã€‚
            use_cache: æ˜¯å¦å¼€å¯ KV Cache åŠ é€Ÿ (æ¨ç†æ—¶ä¸º True)ã€‚

        Returns:
            hidden_states: [B, S, H] æ¨¡å‹è¾“å‡ºç‰¹å¾
            presents: æ–°çš„ KV Cache åˆ—è¡¨
            aux_loss: MoE è´Ÿè½½å‡è¡¡è¾…åŠ©æŸå¤±
        """
        
        # æ³¨æ„ï¼šæ¨ç† Decoding é˜¶æ®µï¼Œseq_length å§‹ç»ˆä¸º 1
        batch_size, seq_length = input_ids.shape

        # ========== KV Cache å…¼å®¹æ€§å¤„ç† ==========
        # å¦‚æœä¼ å…¥çš„æ˜¯ Hugging Face æ–°ç‰ˆçš„é«˜çº§ Cache å¯¹è±¡ (å«æœ‰ .layers å±æ€§)
        # MiniMind æš‚æ—¶ä¸æ”¯æŒï¼Œä¸ºäº†é˜²æ­¢æŠ¥é”™ï¼Œå¼ºåˆ¶æ¸…ç©ºç¼“å­˜ (å®‰å…¨é™çº§)
        if hasattr(past_key_values, 'layers'):
            past_key_values = None

        # åˆå§‹åŒ– past_key_values
        # å¦‚æœæ²¡æœ‰ç¼“å­˜ (Prefill é˜¶æ®µæˆ–è®­ç»ƒé˜¶æ®µ)ï¼Œåˆå§‹åŒ–ä¸ºå…¨ None çš„åˆ—è¡¨
        past_key_values = past_key_values or [None] * len(self.layers)

        # ========== è®¡ç®—èµ·å§‹ä½ç½® (start_pos) ==========
        # è¿™é‡Œçš„é€»è¾‘æ˜¯ç¡®å®šå½“å‰è¾“å…¥çš„ Token åœ¨æ•´ç¯‡æ–‡ç« ä¸­çš„ç»å¯¹ä½ç½®ç´¢å¼•
        # 1. å¦‚æœæœ‰ç¼“å­˜ (past_key_values[0] ä¸ä¸º None):
        #    è¯´æ˜æ˜¯æ¨ç†çš„ Decoding é˜¶æ®µã€‚
        #    past_key_values[0][0] æ˜¯ç¬¬ 0 å±‚çš„ Key Tensorï¼Œå½¢çŠ¶ [B, Past_Len, H_kv, HD]
        #    .shape[1] å°±æ˜¯ Past_Len (å†å²å·²ç»å¤„ç†è¿‡çš„ Token æ•°é‡)
        #    è¿™ä¹Ÿæ˜¯å½“å‰æ–° Token çš„èµ·å§‹ç´¢å¼•ã€‚

        # 2. å¦‚æœæ²¡æœ‰ç¼“å­˜:
        #    è¯´æ˜æ˜¯ Prefill é˜¶æ®µæˆ–è®­ç»ƒé˜¶æ®µï¼Œä»ç¬¬ 0 ä¸ªä½ç½®å¼€å§‹ã€‚
        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0

        # Token Embedding
        hidden_states = self.dropout(self.embed_tokens(input_ids))

        # ========== æå–ä½ç½®ç¼–ç  (RoPE Slicing) ==========
        # æ ¹æ®ç»å¯¹ä½ç½® start_pos å’Œå½“å‰é•¿åº¦ seq_lengthï¼Œä»é¢„è®¡ç®—çš„è¡¨ä¸­åˆ‡ç‰‡
        # åˆ‡ç‰‡èŒƒå›´: [start_pos : start_pos + seq_length]
        # 
        # åœºæ™¯ A (è®­ç»ƒ/Prefill): start_pos=0, seq_len=N -> å–å‡ºå‰ N ä¸ªä½ç½®ç¼–ç 
        # åœºæ™¯ B (æ¨ç† Decoding): start_pos=N, seq_len=1 -> ä»…å–å‡ºç¬¬ N ä¸ªä½ç½®çš„ç¼–ç  (é•¿åº¦ä¸º 1)
        position_embeddings = (
            self.freqs_cos[start_pos:start_pos + seq_length],
            self.freqs_sin[start_pos:start_pos + seq_length]
        )

        # ========== é€å±‚å‰å‘ä¼ æ’­ ==========
        presents = [] # ç”¨äºæ”¶é›†æ¯ä¸€å±‚æ–°çš„ KV Cache

        # zip ç»„åˆï¼šå°† æ¨¡å‹å±‚å¯¹è±¡ ä¸ è¯¥å±‚å¯¹åº”çš„å†å²ç¼“å­˜ é…å¯¹
        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):
            # è¾“å…¥: hidden_states [B, S, H]
            # è¾“å‡º: 
            #   hidden_states: æ›´æ–°åçš„ç‰¹å¾ [B, S, H]
            #   present: å½“å‰å±‚æ›´æ–°åçš„ KV Cache (åŒ…å«å†å²+å½“å‰), å½¢çŠ¶ [B, Past_Len+S, H_kv, HD]
            hidden_states, present = layer(
                hidden_states,
                position_embeddings, # ä¼ å…¥åˆ‡ç‰‡å¥½çš„ä½ç½®ç¼–ç 
                past_key_value=past_key_value, # ä¼ å…¥è¯¥å±‚çš„å†å²ç¼“å­˜
                use_cache=use_cache,
                attention_mask=attention_mask
            )
            presents.append(present)

        # ========== æœ€ç»ˆå½’ä¸€åŒ– ==========
        # ç»è¿‡æ‰€æœ‰å±‚åï¼Œè¿›è¡Œæœ€åä¸€æ¬¡ RMSNorm
        # [B, S, H] -> [B, S, H]
        hidden_states = self.norm(hidden_states)

        # ========== æ±‡æ€» MoE è¾…åŠ©æŸå¤± ==========
        # æ£€æŸ¥æ¯ä¸€å±‚ï¼Œå¦‚æœæ˜¯ MoE å±‚ (MOEFeedForward)ï¼Œæå–å…¶ aux_loss
        # å°†æ‰€æœ‰å±‚çš„ aux_loss ç›¸åŠ ï¼Œç”¨äºè®­ç»ƒæ—¶çš„åå‘ä¼ æ’­
        # å¦‚æœæ²¡æœ‰ä½¿ç”¨ MoEï¼Œæ€» aux_loss ä¸º 0
        aux_loss = sum(
            [l.mlp.aux_loss for l in self.layers if isinstance(l.mlp, MOEFeedForward)],
            hidden_states.new_zeros(1).squeeze()
        )

        return hidden_states, presents, aux_loss


class MiniMindForCausalLM(PreTrainedModel, GenerationMixin):
    # Input IDs -> [MiniMindModel] -> Hidden States -> [LM Head] -> Logits
    """
    1. æƒé‡å…±äº« (Weight Tying): è¾“å…¥ Embedding å’Œ è¾“å‡º LM Head å…±äº«åŒä¸€ä»½å‚æ•°ï¼Œæ˜¾è‘—å‡å°‘æ˜¾å­˜ã€‚
    2. æ¨ç†ä¼˜åŒ– (Logits Slicing): æ”¯æŒåªè®¡ç®—æœ€åä¸€ä¸ª Token çš„ Logitsï¼Œé¿å…å…¨é‡è®¡ç®—ã€‚
    3. è®­ç»ƒå¹¶è¡Œ (Parallel Training): åˆ©ç”¨ Mask å®ç°ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ Token çš„ Lossã€‚
    """
    
    config_class = MiniMindConfig # æŒ‡å®šé…ç½®ç±»ï¼ŒHugging Face æ¡†æ¶è‡ªåŠ¨åŠ è½½æœºåˆ¶éœ€è¦

    def __init__(self, config: MiniMindConfig = None):
        self.config = config or MiniMindConfig()
        # åˆå§‹åŒ–çˆ¶ç±» PreTrainedModel (è´Ÿè´£æƒé‡åŠ è½½ã€ä¿å­˜ã€ä¸‹è½½ç­‰)
        super().__init__(self.config)

        # ========== 1. éª¨å¹²ç½‘ç»œ (Backbone) ==========
        # å®ä¾‹åŒ–çº¯ Transformer Decoder
        # è¾“å…¥: [Batch, Seq_Len] -> è¾“å‡º: [Batch, Seq_Len, Hidden_Size]
        self.model = MiniMindModel(self.config)

        # ========== 2. è¯­è¨€æ¨¡å‹å¤´ (LM Head) ==========
        # è¿™æ˜¯ä¸€ä¸ªçº¿æ€§æŠ•å½±å±‚ (Linear Layer)
        # ä½œç”¨: å°†é«˜ç»´ç‰¹å¾å‘é‡ (Hidden State) æ˜ å°„å›è¯è¡¨ç©ºé—´ (Vocab Space)
        # å½¢çŠ¶: [Hidden_Size] -> [Vocab_Size]
        # bias=False: ç°ä»£å¤§æ¨¡å‹ (LLaMAç­‰) é€šå¸¸ä¸ä½¿ç”¨åç½®é¡¹ï¼Œä»¥æå‡æ•°å€¼ç¨³å®šæ€§
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)

        # ========== 3. æƒé‡å…±äº« (Weight Tying) ==========
        # [é‡è¦ä¼˜åŒ–] å°† Input Embedding çš„æƒé‡æŒ‡é’ˆæŒ‡å‘ LM Head çš„æƒé‡
        # ç‰©ç†æ„ä¹‰: è¯­ä¹‰ä¸Šï¼Œâ€œè¾“å…¥ä¸€ä¸ªè¯â€å’Œâ€œé¢„æµ‹ä¸€ä¸ªè¯â€ä½¿ç”¨çš„æ˜¯åŒä¸€ä¸ªè¯­ä¹‰ç©ºé—´ã€‚
        # æ˜¾å­˜ä¼˜åŠ¿: è¯è¡¨é€šå¸¸å¾ˆå¤§ (å¦‚ 64k)ï¼Œæƒé‡å…±äº«èƒ½èŠ‚çœå¤§é‡å‚æ•° (Hidden * Vocab)ã€‚
        self.model.embed_tokens.weight = self.lm_head.weight # Weight Tying

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                labels: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                logits_to_keep: Union[int, torch.Tensor] = 0,
                **args):
        """
        å‰å‘ä¼ æ’­ (æ”¯æŒ è®­ç»ƒ å’Œ æ¨ç† ä¸¤ç§æ¨¡å¼)
        
        Args:
            input_ids: è¾“å…¥åºåˆ— [Batch, Seq_Len]ã€‚
                       - è®­ç»ƒæ—¶: æ˜¯ä¸€æ•´å¥è¯ (Seq_Len = N)ã€‚
                       - æ¨ç†æ—¶(Decoding): é€šå¸¸åªæ˜¯æœ€æ–°ç”Ÿæˆçš„é‚£ä¸ªè¯ (Seq_Len = 1)ã€‚
            
            attention_mask: æ©ç  [Batch, Seq_Len] (1=æœ‰æ•ˆ, 0=padding)ã€‚
            
            labels: æ ‡ç­¾åºåˆ— [Batch, Seq_Len]ã€‚
                    - å¦‚æœæä¾›æ­¤å‚æ•°ï¼Œæ¨¡å‹ä¼šè®¡ç®— Loss (è®­ç»ƒæ¨¡å¼)ã€‚
                    - å¦‚æœä¸º Noneï¼Œåªè¿”å› Logits (æ¨ç†æ¨¡å¼)ã€‚
            
            past_key_values: KV Cache åˆ—è¡¨ã€‚
                    - ç”¨äºå­˜å‚¨æ¯å±‚çš„å†å² Token çš„ Key/Valueï¼Œé¿å…é‡å¤è®¡ç®—ã€‚
            
            use_cache: æ˜¯å¦è¿”å›æ›´æ–°åçš„ KV Cache (æ¨ç†æ—¶å¼€å¯)ã€‚
            
            logits_to_keep: ã€æ€§èƒ½ä¼˜åŒ–å‚æ•°ã€‘
                    - 0 (é»˜è®¤): è®¡ç®—æ‰€æœ‰ Token çš„ Logits (è®­ç»ƒæ—¶å¿…é¡»é€‰è¿™ä¸ª)ã€‚
                    - 1 (å¸¸ç”¨): åªè®¡ç®—æœ€åä¸€ä¸ª Token çš„ Logits (æ¨ç†ç”Ÿæˆæ—¶ç”¨)ã€‚
                    åŸç†: é¿å…åœ¨ lm_head ä¸Šè¿›è¡Œæ— ç”¨çš„çŸ©é˜µä¹˜æ³•è®¡ç®—ã€‚
        
        Returns:
            CausalLMOutputWithPast: åŒ…å« loss, logits, hidden_states, past_key_values, aux_loss
        """

        # ========== æ­¥éª¤ 1: éª¨å¹²ç½‘ç»œç‰¹å¾æå– ==========
        # hidden_states: [Batch, Seq_Len, Hidden_Size]
        # past_key_values: åŒ…å«äº†å½“å‰æ­¥æ–°ç”Ÿæˆçš„ KV Cache
        # aux_loss: å¦‚æœä½¿ç”¨äº† MoEï¼Œè¿™é‡Œä¼šè¿”å›è´Ÿè½½å‡è¡¡æŸå¤±ï¼›å¦åˆ™ä¸º 0
        hidden_states, past_key_values, aux_loss = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **args
        )

        # ========== æ­¥éª¤ 2: Logits è®¡ç®—èŒƒå›´ä¼˜åŒ– (Logits Slicing) ==========
        # lm_head çš„è®¡ç®—é‡æ˜¯ O(Seq_Len * Hidden * Vocab)ï¼Œéå¸¸å·¨å¤§ã€‚
        # åœ¨æ¨ç†æ—¶ï¼Œæˆ‘ä»¬åªéœ€è¦æœ€åä¸€ä¸ªè¯çš„é¢„æµ‹ç»“æœï¼Œä¸éœ€è¦å‰æ–‡çš„é¢„æµ‹ã€‚

        if isinstance(logits_to_keep, int):
            # logits_to_keep æ˜¯æ•´æ•°
            # logits_to_keep = 1 -> slice(-1, None) -> å–æœ€å 1 ä¸ª
            # logits_to_keep = 0 -> slice(None)     -> å–å…¨éƒ¨ (è®­ç»ƒæ—¶)
            slice_indices = slice(-logits_to_keep, None) if logits_to_keep > 0 else slice(None)
        else:
            # logits_to_keep æ˜¯å¼ é‡ (é«˜çº§ç”¨æ³•ï¼ŒæŒ‡å®šç‰¹å®šä½ç½®)
            slice_indices = logits_to_keep

        # å¯¹ Hidden States è¿›è¡Œåˆ‡ç‰‡ï¼Œåªä¿ç•™éœ€è¦è®¡ç®—çš„éƒ¨åˆ†
        # æ¨ç†æ—¶: [Batch, 100, Hidden] -> [Batch, 1, Hidden]
        # è®­ç»ƒæ—¶: [Batch, 100, Hidden] -> [Batch, 100, Hidden]
        sliced_hidden_states = hidden_states[:, slice_indices, :]
        
        # ========== æ­¥éª¤ 3: æ˜ å°„åˆ°è¯è¡¨ (Projection) ==========
        # æ‰§è¡ŒçŸ©é˜µä¹˜æ³•: X @ W.T
        # logits å½¢çŠ¶: [Batch, Sliced_Len, Vocab_Size]
        # è¿™é‡Œçš„ logits æ˜¯æœªå½’ä¸€åŒ–çš„æ¦‚ç‡ (Log-odds)
        logits = self.lm_head(sliced_hidden_states)

        # ========== æ­¥éª¤ 4: è®¡ç®—æŸå¤± (ä»…è®­ç»ƒæ¨¡å¼) ==========
        loss = None
        if labels is not None:
            # å› æœè¯­è¨€æ¨¡å‹çš„æ ¸å¿ƒé€»è¾‘: "Shift Prediction" (ä½ç§»é¢„æµ‹)
            # ç›®æ ‡: ç¬¬ t ä¸ªæ—¶é—´æ­¥çš„ Logitï¼Œåº”è¯¥é¢„æµ‹ç¬¬ t+1 ä¸ªæ—¶é—´æ­¥çš„ Labelã€‚
            
            # [Input]:  A  B  C  | D (ä¸¢å¼ƒæœ€åä¸€ä¸ª Tokenï¼Œå› ä¸ºå®ƒæ²¡æœ‰å¯¹åº”çš„ Label)
            # [Target]: B  C  D  | E  (ä¸¢å¼ƒç¬¬ä¸€ä¸ª Labelï¼Œå› ä¸ºå®ƒæ²¡æœ‰å¯¹åº”çš„ Logit)
            
            # shift_logits: å»æ‰æœ€åä¸€ä¸ª Logit (å› ä¸ºå®ƒé¢„æµ‹çš„æ˜¯ Eï¼Œä½† Input åªæœ‰åˆ° Dï¼Œæ²¡æœ‰å¯¹åº”çš„ label)
            # å½¢çŠ¶: [Batch, Seq_Len-1, Vocab]
            shift_logits = logits[..., :-1, :].contiguous()
            
            # shift_labels: å»æ‰ç¬¬ä¸€ä¸ª Label (å› ä¸º A ä¹‹å‰æ²¡æœ‰ Logit é¢„æµ‹å®ƒï¼Œæ²¡æœ‰å¯¹åº”çš„ logit)
            # å½¢çŠ¶: [Batch, Seq_Len-1]
            shift_labels = labels[..., 1:].contiguous()

            # è®¡ç®—äº¤å‰ç†µæŸå¤± (Cross Entropy)
            # ignore_index=-100: å¿½ç•¥æ ‡ç­¾ä¸º -100 (Padding) çš„ä½ç½®ï¼Œä¸è®¡ç®—æ¢¯åº¦
            loss = F.cross_entropy(
                shift_logits.view(-1, shift_logits.size(-1)), # view(-1): å°† Batch å’Œ Seq ç»´åº¦å±•å¹³ï¼Œå˜æˆ [Total_Tokens, Vocab]
                shift_labels.view(-1), # [Total_Tokens]
                ignore_index=-100
            )

        # ========== æ­¥éª¤ 5: å°è£…è¾“å‡º ==========
        # ä½¿ç”¨ Hugging Face æ ‡å‡†æ ¼å¼è¿”å›ï¼Œç¡®ä¿å…¼å®¹æ€§
        output = CausalLMOutputWithPast(loss=loss, logits=logits, past_key_values=past_key_values, hidden_states=hidden_states)

        # [MoE ç‰¹æœ‰] æŒ‚è½½è¾…åŠ©æŸå¤±
        # è®­ç»ƒå¾ªç¯ä¸­é€šå¸¸å†™æ³•: total_loss = output.loss + alpha * output.aux_loss
        output.aux_loss = aux_loss

        return output
